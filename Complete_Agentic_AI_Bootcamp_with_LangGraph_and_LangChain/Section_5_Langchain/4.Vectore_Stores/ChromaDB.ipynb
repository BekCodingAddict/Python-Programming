{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1526bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python-Programming\\Complete_Agentic_AI_Bootcamp_with_LangGraph_and_LangChain\\Section_5_Langchain\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## build a sample vector store\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c16249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.\\nSources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.\\nFormats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).\\nDestination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".\\nTransformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed. \\nTypes of Data Ingestion\\nThe method chosen depends on business needs and the required speed of access: \\nBatch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.\\nReal-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.\\nHybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs. \\nImportance\\nEffective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions. \\nWant me to explain the specific tools or challenges involved in data ingestion?')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=TextLoader(\"What_is_DataIngestion.txt\")\n",
    "documents=loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe35b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=0)\n",
    "splits=text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd985a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hello\\AppData\\Local\\Temp\\ipykernel_10296\\2722929055.py:1: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings=OllamaEmbeddings(model=\"nomic-embed-text\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x17a6990f6d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings=OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "vectordb=Chroma.from_documents(documents=splits,embedding=embeddings)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb873b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Querying the Vector Store\n",
    "query=\"What is Data Ingestion?\"\n",
    "docs=vectordb.similarity_search(query)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc856934",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save to the disk\n",
    "vectordb=Chroma.from_documents(documents=splits,embedding=embeddings,persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141550da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Call the vector store\n",
    "db2=Chroma(persist_directory=\"./chroma_db\",embedding_function=embeddings)\n",
    "docs=db2.similarity_search(\"What is Data Ingestion?\")\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3f4433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retriever option\n",
    "retriver=vectordb.as_retriever()\n",
    "retriver.invoke(query)[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24b621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
