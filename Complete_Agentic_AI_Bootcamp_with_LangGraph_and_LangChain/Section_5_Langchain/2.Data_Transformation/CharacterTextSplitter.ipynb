{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1710c5d4",
   "metadata": {},
   "source": [
    "## Character Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c65ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.\\nSources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.\\nFormats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).\\nDestination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".\\nTransformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed. \\nTypes of Data Ingestion\\nThe method chosen depends on business needs and the required speed of access: \\nBatch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.\\nReal-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.\\nHybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs. \\nImportance\\nEffective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions. \\nWant me to explain the specific tools or challenges involved in data ingestion?')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"What_is_DataIngestion.txt\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf525598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 295, which is longer than the specified 100\n",
      "Created a chunk of size 164, which is longer than the specified 100\n",
      "Created a chunk of size 152, which is longer than the specified 100\n",
      "Created a chunk of size 187, which is longer than the specified 100\n",
      "Created a chunk of size 153, which is longer than the specified 100\n",
      "Created a chunk of size 227, which is longer than the specified 100\n",
      "Created a chunk of size 176, which is longer than the specified 100\n",
      "Created a chunk of size 270, which is longer than the specified 100\n",
      "Created a chunk of size 101, which is longer than the specified 100\n",
      "Created a chunk of size 213, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Key Aspects'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Purpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Sources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Formats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Destination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Transformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Types of Data Ingestion'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='The method chosen depends on business needs and the required speed of access:'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Batch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Real-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Hybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Importance'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Effective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions.'),\n",
       " Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Want me to explain the specific tools or challenges involved in data ingestion?')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter=CharacterTextSplitter(separator=\"\\n\" , chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88c82bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \n",
      "Key Aspects\n",
      "Purpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.\n",
      "Sources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.\n",
      "Formats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).\n",
      "Destination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".\n",
      "Transformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed. \n",
      "Types of Data Ingestion\n",
      "The method chosen depends on business needs and the required speed of access: \n",
      "Batch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.\n",
      "Real-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.\n",
      "Hybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs. \n",
      "Importance\n",
      "Effective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions. \n",
      "Want me to explain the specific tools or challenges involved in data ingestion?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text=\"\"\n",
    "with open(\"What_is_DataIngestion.txt\") as file:\n",
    "    text=file.read()\n",
    "\n",
    "text_splitter=CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text=text_splitter.create_documents([text])\n",
    "print(text[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
