{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc3a3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'title': 'sample', 'author': 'Philip Hutchison', 'moddate': \"D:20080701052447Z00'00'\", 'source': 'sample.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='Sample PDFThis is a simple PDF ﬁle. Fun fun fun.\\nLorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \\nCurabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \\npharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \\nInteger a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \\nVestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla \\nerat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \\nVivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \\naccumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \\nhendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \\nSuspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \\npulvinar quis, nisl.\\nPellentesque sit amet lectus. Praesent pulvinar, nunc quis iaculis sagittis, justo quam \\nlobortis tortor, sed vestibulum dui metus venenatis est. Nunc cursus ligula. Nulla facilisi. \\nPhasellus ullamcorper consectetuer ante. Duis tincidunt, urna id condimentum luctus, nibh \\nante vulputate sapien, id sagittis massa orci ut enim. Pellentesque vestibulum convallis \\nsem. Nulla consequat quam ut nisl. Nullam est. Curabitur tincidunt dapibus lorem. Proin \\nvelit turpis, scelerisque sit amet, iaculis nec, rhoncus ac, ipsum. Phasellus lorem arcu, \\nfeugiat eu, gravida eu, consequat molestie, ipsum. Nullam vel est ut ipsum volutpat \\nfeugiat. Aenean pellentesque.\\nIn mauris. Pellentesque dui nisi, iaculis eu, rhoncus in, venenatis ac, ante. Ut odio justo, \\nscelerisque vel, facilisis non, commodo a, pede. Cras nec massa sit amet tortor volutpat \\nvarius. Donec lacinia, neque a luctus aliquet, pede massa imperdiet ante, at varius lorem \\npede sed sapien. Fusce erat nibh, aliquet in, eleifend eget, commodo eget, erat. Fusce \\nconsectetuer. Cras risus tortor, porttitor nec, tristique sed, convallis semper, eros. Fusce \\nvulputate ipsum a mauris. Phasellus mollis. Curabitur sed urna. Aliquam nec sapien non \\nnibh pulvinar convallis. Vivamus facilisis augue quis quam. Proin cursus aliquet metus. \\nSuspendisse lacinia. Nulla at tellus ac turpis eleifend scelerisque. Maecenas a pede vitae \\nenim commodo interdum. Donec odio. Sed sollicitudin dui vitae justo.\\nMorbi elit nunc, facilisis a, mollis a, molestie at, lectus. Suspendisse eget mauris eu tellus \\nmolestie cursus. Duis ut magna at justo dignissim condimentum. Cum sociis natoque \\npenatibus et magnis dis parturient montes, nascetur ridiculus mus. Vivamus varius. Ut sit \\namet diam suscipit mauris ornare aliquam. Sed varius. Duis arcu. Etiam tristique massa \\neget dui. Phasellus congue. Aenean est erat, tincidunt eget, venenatis quis, commodo at, \\nquam.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading a PDF file\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader=PyPDFLoader(file_path=\"sample.pdf\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8839791b",
   "metadata": {},
   "source": [
    "### How to recursively split the text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a48f6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.'), Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Sources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.\\nFormats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).\\nDestination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".'), Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Transformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed. \\nTypes of Data Ingestion\\nThe method chosen depends on business needs and the required speed of access:'), Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Batch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.\\nReal-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.'), Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Hybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs. \\nImportance\\nEffective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions. \\nWant me to explain the specific tools or challenges involved in data ingestion?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "final_docs=text_splitter.split_documents(docs)\n",
    "print(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27544bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Sample PDFThis is a simple PDF ﬁle. Fun fun fun.\n",
      "Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Phasellus facilisis odio sed mi. \n",
      "Curabitur suscipit. Nullam vel nisi. Etiam semper ipsum ut lectus. Proin aliquam, erat eget \n",
      "pharetra commodo, eros mi condimentum quam, sed commodo justo quam ut velit. \n",
      "Integer a erat. Cras laoreet ligula cursus enim. Aenean scelerisque velit et tellus. \n",
      "Vestibulum dictum aliquet sem. Nulla facilisi. Vestibulum accumsan ante vitae elit. Nulla' metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'title': 'sample', 'author': 'Philip Hutchison', 'moddate': \"D:20080701052447Z00'00'\", 'source': 'sample.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n",
      "page_content='erat dolor, blandit in, rutrum quis, semper pulvinar, enim. Nullam varius congue risus. \n",
      "Vivamus sollicitudin, metus ut interdum eleifend, nisi tellus pellentesque elit, tristique \n",
      "accumsan eros quam et risus. Suspendisse libero odio, mattis sit amet, aliquet eget, \n",
      "hendrerit vel, nulla. Sed vitae augue. Aliquam erat volutpat. Aliquam feugiat vulputate nisl. \n",
      "Suspendisse quis nulla pretium ante pretium mollis. Proin velit ligula, sagittis at, egestas a, \n",
      "pulvinar quis, nisl.' metadata={'producer': 'Mac OS X 10.5.4 Quartz PDFContext', 'creator': 'Pages', 'creationdate': \"D:20080701052447Z00'00'\", 'title': 'sample', 'author': 'Philip Hutchison', 'moddate': \"D:20080701052447Z00'00'\", 'source': 'sample.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(final_docs[0])\n",
    "print(final_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2b14280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'What_is_DataIngestion.txt'}, page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources into a centralized destination (such as a data lake, data warehouse, or database) where it can be stored, processed, and analyzed. It is the foundational first step in any modern data pipeline. \\nKey Aspects\\nPurpose: The main goal is to efficiently gather data and make it available and accessible for business intelligence, analytics, and machine learning/AI initiatives.\\nSources: Data can come from a wide variety of sources, including databases, APIs, log files, IoT devices, applications (SaaS), and file storage systems.\\nFormats: The process handles data in various formats, including structured (like databases), semi-structured (like JSON or XML), and unstructured data (like images, audio, or text files).\\nDestination: The data is typically moved into a single repository for consolidation, allowing organizations to create a unified \"single source of truth\".\\nTransformation: Unlike the traditional Extract, Transform, Load (ETL) process where data is transformed before loading, data ingestion often loads the raw data first (as in ELT), with transformations happening later as needed. \\nTypes of Data Ingestion\\nThe method chosen depends on business needs and the required speed of access: \\nBatch Ingestion: Data is collected and moved in large chunks at regular, scheduled intervals (e.g., daily or hourly). This is suitable when real-time insights are not critical.\\nReal-time (Streaming) Ingestion: Data is continuously collected and moved as soon as it is generated, with very low latency. This is essential for scenarios requiring immediate, up-to-the-minute analysis, such as monitoring stock market trades or IoT device performance.\\nHybrid Ingestion: Combines both batch and real-time approaches to meet diverse organizational needs. \\nImportance\\nEffective data ingestion is vital because downstream data science, business intelligence, and analytics systems rely heavily on timely, complete, and accurate data to drive innovation and make informed decisions. \\nWant me to explain the specific tools or challenges involved in data ingestion?')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"What_is_DataIngestion.txt\")\n",
    "docs=loader.load()\n",
    "print(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64a4688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Data ingestion is the process of collecting and importing raw data from multiple and diverse sources'\n",
      "page_content='and diverse sources into a centralized destination (such as a data lake, data warehouse, or'\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text=\"\"\n",
    "with open(\"What_is_DataIngestion.txt\") as file:\n",
    "    text=file.read()\n",
    "\n",
    "#print(text)\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=20)\n",
    "text=text_splitter.create_documents([text])\n",
    "#print(text)\n",
    "print(text[0])\n",
    "print(text[1])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37d321",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
