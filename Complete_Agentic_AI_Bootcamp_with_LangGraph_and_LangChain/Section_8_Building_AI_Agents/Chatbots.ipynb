{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f17282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41a8e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\Python-Programming\\Complete_Agentic_AI_Bootcamp_with_LangGraph_and_LangChain\\Section_8_Building_AI_Agents\\venv\\lib\\site-packages\\pydantic\\main.py:250: UserWarning: WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001A079586C20>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001A079586B30>, model_name='openai/gpt-oss-20b', temperature=0.5, model_kwargs={'top_p': 0.95}, groq_api_key=SecretStr('**********'), max_tokens=8192)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(\n",
    "    model_name=\"openai/gpt-oss-20b\",\n",
    "    api_key=groq_api_key,\n",
    "    temperature=0.5,\n",
    "    max_tokens=8192,\n",
    "    top_p=0.95,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9060821c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! I'm doing great‚Äîthanks for asking. How about you? What‚Äôs on your mind today?\", additional_kwargs={'reasoning_content': 'The user says \"Hello, how are you?\" It\\'s a greeting. The assistant should respond politely. There\\'s no special instructions. So respond with a friendly greeting and ask how user is.'}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 77, 'total_tokens': 145, 'completion_time': 0.066149032, 'completion_tokens_details': {'reasoning_tokens': 38}, 'prompt_time': 0.004282837, 'prompt_tokens_details': None, 'queue_time': 0.004566388, 'total_time': 0.070431869}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_75c733514d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--bd411fc6-e2b6-4bdc-8e68-5e499aed3f12-0', usage_metadata={'input_tokens': 77, 'output_tokens': 68, 'total_tokens': 145, 'output_token_details': {'reasoning': 38}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b4c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre John\\u202fDoe, 30\\u202fyears old, and you live in San\\u202fFrancisco.', additional_kwargs={'reasoning_content': 'We need to answer based on the user-provided info. The user says: \"Hello, My name is John Doe.I am a software engineer.I am 30 years old.I live in San Francisco.I am a good person.\" So name: John Doe, age: 30, location: San Francisco. Provide that.'}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 135, 'total_tokens': 231, 'completion_time': 0.094303803, 'completion_tokens_details': {'reasoning_tokens': 66}, 'prompt_time': 0.007453729, 'prompt_tokens_details': None, 'queue_time': 0.004640172, 'total_time': 0.101757532}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_75c733514d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--ad410f21-346b-4214-8ae8-ca96718f1eb1-0', usage_metadata={'input_tokens': 135, 'output_tokens': 96, 'total_tokens': 231, 'output_token_details': {'reasoning': 66}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([\n",
    "    HumanMessage(content=\"Hello, My name is John Doe.I am a software engineer.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "    AIMessage(content=\"Hello, John Doe. How can I help you today?\"),\n",
    "    HumanMessage(content=\"Hey What is my name and age? Where do I live?\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde91ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcd17f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, John! It‚Äôs great to meet you. üëã\\n\\n- Software engineer at 30? That‚Äôs a solid combination of experience and fresh perspective.  \\n- Living in San\\u202fFrancisco‚Äîso you‚Äôre probably surrounded by some of the coolest tech scenes and amazing food spots.  \\n- And you say you‚Äôre a good person‚Äînice to hear that!\\n\\nAnything in particular you‚Äôd like to chat about? Maybe a project you‚Äôre excited about, a favorite SF spot, or just a random tech question? I‚Äôm all ears!', additional_kwargs={'reasoning_content': 'We need to respond. The user says \"Hello, My name is John Doe.I am a software engineer.I am 30 years old.I live in San Francisco.I am a good person.\" They likely want a friendly response. There\\'s no request. We can respond acknowledging and maybe ask follow-up. Ensure no policy violation. It\\'s safe.'}, response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 100, 'total_tokens': 286, 'completion_time': 0.184187575, 'completion_tokens_details': {'reasoning_tokens': 68}, 'prompt_time': 0.00553687, 'prompt_tokens_details': None, 'queue_time': 0.004460766, 'total_time': 0.189724445}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_a12402de73', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--3b8b3ba4-d1ec-4f89-a5e0-78fcd3c495ab-0', usage_metadata={'input_tokens': 100, 'output_tokens': 186, 'total_tokens': 286, 'output_token_details': {'reasoning': 68}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}\n",
    "with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hello, My name is John Doe.I am a software engineer.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d11e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You‚Äôre John\\u202fDoe, 30\\u202fyears old, and you live in San\\u202fFrancisco.', additional_kwargs={'reasoning_content': 'User: \"Hey What is my name and age? Where do I live?\" We have the conversation: user introduced themselves: \"Hello, My name is John Doe. I am a software engineer. I am 30 years old. I live in San Francisco. I am a good person.\" So we can answer: name John Doe, age 30, live in San Francisco. We should be concise.'}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 232, 'total_tokens': 345, 'completion_time': 0.11068914, 'completion_tokens_details': {'reasoning_tokens': 83}, 'prompt_time': 0.011291267, 'prompt_tokens_details': None, 'queue_time': 0.004710182, 'total_time': 0.121980407}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e23fc997ca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--d8c612ae-b3b4-4b14-a1ea-b0d9f6872e11-0', usage_metadata={'input_tokens': 232, 'output_tokens': 113, 'total_tokens': 345, 'output_token_details': {'reasoning': 83}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey What is my name and age? Where do I live?\"),\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003783a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I‚Äôm not sure what your name and age are. Could you let me know?', additional_kwargs={'reasoning_content': 'User asks \"What is my name and age?\" They didn\\'t provide name or age. According to policy, we can\\'t guess. We should ask them. So we respond asking for name and age.'}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 78, 'total_tokens': 144, 'completion_time': 0.064427379, 'completion_tokens_details': {'reasoning_tokens': 40}, 'prompt_time': 0.003630861, 'prompt_tokens_details': None, 'queue_time': 0.004707465, 'total_time': 0.06805824}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_75c733514d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--51f8d4c3-e762-4b9d-88e7-3478673ce65b-0', usage_metadata={'input_tokens': 78, 'output_tokens': 66, 'total_tokens': 144, 'output_token_details': {'reasoning': 40}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Changing session id\n",
    "config1={\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"What is my name and age?\"),\n",
    "    config=config1\n",
    ")\n",
    "\n",
    "response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02aaedfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, John! Thanks for sharing a bit about yourself. How can I help you today?', additional_kwargs={'reasoning_content': 'User says: \"Hey My name is John Doe. I am 30 years old. I live in San Francisco. I am a good person.\" The user is providing personal info. The user might want the assistant to respond. The user didn\\'t ask a question. They just gave info. We should respond acknowledging. Possibly ask if they want anything else. No policy violation. Just respond.'}, response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 128, 'total_tokens': 238, 'completion_time': 0.107279124, 'completion_tokens_details': {'reasoning_tokens': 79}, 'prompt_time': 0.006129147, 'prompt_tokens_details': None, 'queue_time': 0.004446477, 'total_time': 0.113408271}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_75c733514d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--9070589c-dd1c-4475-a617-928aefc720f3-0', usage_metadata={'input_tokens': 128, 'output_tokens': 110, 'total_tokens': 238, 'output_token_details': {'reasoning': 79}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8461995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is John\\u202fDoe and you‚Äôre 30\\u202fyears old.', additional_kwargs={'reasoning_content': 'We have a conversation. The user asks \"What is my name and age?\" The assistant previously was told by the user: \"Hey My name is John Doe. I am 30 years old. I live in San Francisco. I am a good person.\" So the assistant should respond with \"Your name is John Doe and you are 30 years old.\" There\\'s no policy violation. So just answer.'}, response_metadata={'token_usage': {'completion_tokens': 106, 'prompt_tokens': 167, 'total_tokens': 273, 'completion_time': 0.104000724, 'completion_tokens_details': {'reasoning_tokens': 82}, 'prompt_time': 0.008035977, 'prompt_tokens_details': None, 'queue_time': 0.004442818, 'total_time': 0.112036701}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_75c733514d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--b0f4c0b7-58e9-40d2-8f33-5ce9461cdab3-0', usage_metadata={'input_tokens': 167, 'output_tokens': 106, 'total_tokens': 273, 'output_token_details': {'reasoning': 82}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"What is my name and age?\"),\n",
    "    config=config1\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30445532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions.\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc61e6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, John! üëã How can I help you today? Whether it‚Äôs a quick question, a deep dive into a topic, or just a chat, I‚Äôm here for you.', additional_kwargs={'reasoning_content': \"We need to respond. The user introduced themselves. We should respond politely, maybe ask a question or offer help. There's no specific question. We can respond acknowledging.\"}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 107, 'total_tokens': 191, 'completion_time': 0.08685892, 'completion_tokens_details': {'reasoning_tokens': 34}, 'prompt_time': 0.008647409, 'prompt_tokens_details': None, 'queue_time': 0.050733351, 'total_time': 0.095506329}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--d3c8921d-ea7a-484e-8d34-d1416ed1e720-0', usage_metadata={'input_tokens': 107, 'output_tokens': 84, 'total_tokens': 191, 'output_token_details': {'reasoning': 34}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31bdf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "956189cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, John! üëã It‚Äôs great to meet you. How can I help you today? Whether you‚Äôre looking for information, advice, or just want to chat, I‚Äôm here for you.', additional_kwargs={'reasoning_content': 'User says: \"Hey My name is John Doe. I am 30 years old. I live in San Francisco. I am a good person.\" This is a user introduction. We should respond appropriately. There\\'s no question, just introduction. We can respond with greeting, ask about what they need help with. Also we should follow policy: no personal data usage beyond what\\'s provided. We can respond with friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 107, 'total_tokens': 241, 'completion_time': 0.131486651, 'completion_tokens_details': {'reasoning_tokens': 84}, 'prompt_time': 0.00516118, 'prompt_tokens_details': None, 'queue_time': 0.004539998, 'total_time': 0.136647831}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e23fc997ca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--6cba0175-af77-4234-b4f8-dd7e686fad70-0', usage_metadata={'input_tokens': 107, 'output_tokens': 134, 'total_tokens': 241, 'output_token_details': {'reasoning': 84}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b99eeae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÏïàÎÖïÌïòÏÑ∏Ïöî, John DoeÎãò!  \\nÏ†ÄÎäî Ïó¨Îü¨Î∂ÑÏù¥ Í∂ÅÍ∏àÌïú Ï†êÏù¥ÎÇò ÎèÑÏõÄÏù¥ ÌïÑÏöîÌïòÏã† Î∂ÄÎ∂ÑÏùÑ Ïñ∏Ï†úÎì† ÎèÑÏôÄÎìúÎ¶¥ Ï§ÄÎπÑÍ∞Ä ÎêòÏñ¥ ÏûàÏñ¥Ïöî.  \\nÎ¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî? üòä'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"Hey My name is John Doe. I am 30 years old. I live in San Francisco. I am a good person.\"\n",
    "        )\n",
    "    ],\n",
    "    \"language\": \"Korean\"\n",
    "})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c0af58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Salom, John! üëã\\n\\nSizni tanishganimdan xursandman. San-Francisco‚Äôda yashash, 30 yoshda bo‚Äòlish va ‚Äúyaxshi odam‚Äù deb o‚Äòzingizni his qilishingiz ajoyib! Sizga qanday yordam bera olaman? Savollaringiz, maslahatlaringiz yoki istalgan mavzu bo‚Äòlsa, bemalol so‚Äòrashingiz mumkin.', additional_kwargs={'reasoning_content': 'The user repeats the same statement. They likely want a response. According to developer instructions: \"You are a helpful assistant that can answer questions in Uzbek.\" The user wrote in English. We should respond in Uzbek. The user says \"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\" We can respond in Uzbek acknowledging. Provide friendly reply.'}, response_metadata={'token_usage': {'completion_tokens': 183, 'prompt_tokens': 255, 'total_tokens': 438, 'completion_time': 0.198851263, 'completion_tokens_details': {'reasoning_tokens': 82}, 'prompt_time': 0.024248474, 'prompt_tokens_details': None, 'queue_time': 0.055069535, 'total_time': 0.223099737}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--c8f346c2-81c5-4c69-ab68-a61e2551f8f0-0', usage_metadata={'input_tokens': 255, 'output_tokens': 183, 'total_tokens': 438, 'output_token_details': {'reasoning': 82}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain, get_session_history,input_messages_key=\"messages\")\n",
    "congig={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\")],\n",
    "     \"language\":\"Uzbek\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9e69d",
   "metadata": {},
   "source": [
    "## Managing The Conversation History\n",
    "One importtant concept to understand when building chatbots is how to manage conversation history. If left unmanaged . the list of message will grow unbounded and potentiallu overflow the context window of the LLM . Therefore , it is importtant to add a step that limits the size of the message you are passing in. \n",
    "\n",
    "### trim_messages\n",
    "Helper to reduce how many massages we'are sending to model. The trimmer allows us to specify how many token we want to keep, alonmg with other parametrs like if we want to always keep the system message and whether to allow partial message.\n",
    "```python\n",
    "from langchain_core.messages import  trim_messages\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efd977a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that can answer questions.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hey What is my name and age? Where do I live?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey Your name is John Doe and you are 30 years old. You live in San Francisco.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What kind of person am I?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You are a good person.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer=trim_messages(max_tokens=70,strategy=\"last\",token_counter=model,include_system=True,allow_partial=True,start_on=\"human\")\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant that can answer questions.\"),\n",
    "    HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "    AIMessage(content=\"Hello, John Doe. How can I help you today?\"),\n",
    "    HumanMessage(content=\"Hey What is my name and age? Where do I live?\"),\n",
    "    AIMessage(content=\"Hey Your name is John Doe and you are 30 years old. You live in San Francisco.\"),\n",
    "    HumanMessage(content=\"What kind of person am I?\"),\n",
    "    AIMessage(content=\"You are a good person.\")\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8f351d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I‚Äôm not sure I know enough about you to give a precise answer. Could you share a bit about your interests, habits, or how you usually approach situations? That would help me paint a clearer picture of the kind of person you are.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain=(RunnablePassthrough.assign(\n",
    "    messages=itemgetter(\"messages\")|trimmer)|prompt|model)\n",
    "\n",
    "response=chain.invoke({\"messages\":messages+[HumanMessage(content=\"What kind of person am I?\")],\"language\":\"Uzbek\"})\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de621d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Siz 30 yoshli, San‚ÄëFransisko shahrida yashovchi, yaxshi niyatli va o‚Äòxshash inson sifatida tanilgan. Bu ma‚Äôlumotlar asosida quyidagilarni aytish mumkin:\\n\\n- **Ijtimoiy va do‚Äòstona** ‚Äì San‚ÄëFransisko kabi ko‚Äòp madaniyatli shaharda yashash, odamlar bilan aloqada bo‚Äòlishni va turli fikrlarni hurmat qilishni o‚Äòrgatadi.\\n- **O‚Äòziga ishongan** ‚Äì 30 yoshda bo‚Äòlish, hayotning bir necha bosqichini boshdan kechirish va o‚Äòz yo‚Äòlini topish imkonini beradi.\\n- **Yaxshi niyatli** ‚Äì ‚ÄúYaxshi inson‚Äù deb o‚Äòzingizni ta'riflaganingiz, boshqalarga yordam berishga tayyor ekanligingizni ko‚Äòrsatadi.\\n- **Moslashuvchan** ‚Äì San‚ÄëFransiskoning dinamik shahar hayoti, yangi texnologiyalar va imkoniyatlar bilan tanishish, sizni o‚Äòzgarishlarga ochiq qiladi.\\n\\nAlbatta, bu umumiy tavsif bo‚Äòlib, har bir insonning shaxsiyati juda murakkab va ko‚Äòp qirralidir. Sizning haqiqiy shaxsiyatingiz, tajribangiz, qiziqishlaringiz va qadriyatlaringiz bilan yanada boy va o‚Äòziga xos bo‚Äòladi.\", additional_kwargs={'reasoning_content': 'We need to respond in Uzbek. The user says: \"Hey My name is John Doe. I am 30 years old. I live in San Francisco. I am a good person.\" Then asks: \"What kind of person am I?\" We should answer in Uzbek. We can interpret: They want a description of their personality. We can respond: \"Siz yaxshi inson, 30 yosh, San-Fransisko yashovchi, yaxshi xulq-atvorga ega.\" We can add some general traits: \"Sizning yosh, yashash joyi, va xulq-atvoringizga qarab, ehtimol, ijtimoiy, o\\'ziga ishongan, do\\'stona, va yurtga xizmat qiluvchi inson bo\\'lasiz.\" Provide friendly tone.'}, response_metadata={'token_usage': {'completion_tokens': 494, 'prompt_tokens': 119, 'total_tokens': 613, 'completion_time': 0.521558845, 'completion_tokens_details': {'reasoning_tokens': 164}, 'prompt_time': 0.007872154, 'prompt_tokens_details': None, 'queue_time': 0.047900705, 'total_time': 0.529430999}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_e99e93f2ac', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--6ea4c6eb-bdc9-497d-8ba1-14034cb217a9-0', usage_metadata={'input_tokens': 119, 'output_tokens': 494, 'total_tokens': 613, 'output_token_details': {'reasoning': 164}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")\n",
    "config={\"configurable\":{\"session_id\":\"chat5\"}}\n",
    "response=with_message_history.invoke(\n",
    "    {\"messages\":[HumanMessage(content=\"Hey My name is John Doe.I am 30 years old.I live in San Francisco.I am a good person.\"),\n",
    "     HumanMessage(content=\"What kind of person am I?\")]\n",
    "    ,\"language\":\"Uzbek\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd065cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
